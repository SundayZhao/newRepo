{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca4cce3-8eb9-4781-b09e-0c5c0321c275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#必要的库\n",
    "pip install transformers\n",
    "pip install evaluate\n",
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692afeca-3d76-4b26-a43f-5901d0648c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#java数据集生成\n",
    "#对应“java”文件夹\n",
    "import csv\n",
    "#sstubs.csv的文件位置\n",
    "csv_path='.\\datasets\\java\\sstubs-miner\\sstubs.csv'\n",
    "#保存数据集的位置\n",
    "output_buggy='.\\java.buggy'\n",
    "output_fixxed='.\\java.fixed'\n",
    "\n",
    "csv.field_size_limit(500 * 1024 * 1024)\n",
    "with open(csv_path,'r',encoding='utf-8') as f,open (output_buggy,'w',encoding='utf-8') as buggy ,open (output_fixxed,'w',encoding='utf-8') as fixed:\n",
    "  csv_reader = csv.DictReader(f)\n",
    "  for line in csv_reader:\n",
    "    if line['bug_source']!='' and line['fix_source']!='fix_source':\n",
    "      line['bug_source']=line['bug_source'].replace('\\n','')\n",
    "      line['bug_source']=line['bug_source'].replace('\\r','')\n",
    "      line['fix_source']=line['fix_source'].replace('\\n','')\n",
    "      line['fix_source']=line['fix_source'].replace('\\r','')\n",
    "      buggy.write(line['bug_source']+'\\n')\n",
    "      fixed.write(line['fix_source']+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e5bf6-bebf-4926-a26d-982a0c73cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#java_codesearchnet数据集生成\n",
    "#对应“java_codesearchnet”文件夹\n",
    "import os\n",
    "import json\n",
    "jsonl_path='.\\datasets\\java_codesearchnet'\n",
    "jsonl_files = os.listdir(jsonl_path)\n",
    "output_file='.\\java.code.codesearchnet'\n",
    "output_file=open(output_file,'w',encoding='utf-8')\n",
    "for jsonl_file in jsonl_files:\n",
    "  if os.path.splitext(jsonl_file)[1]  != '.jsonl':\n",
    "    continue\n",
    "  f=open(jsonl_file,'r',encoding='utf-8')\n",
    "  code_lines=f.readlines()\n",
    "  print(jsonl_file,len(code_lines))\n",
    "  for code_line in code_lines:\n",
    "    line_json=json.loads(code_line)\n",
    "    code_tokens=line_json['code_tokens']\n",
    "    code=' '.join(code_tokens)\n",
    "    code=code.replace('\\n','')\n",
    "    code=code.replace('\\r','')\n",
    "    output_file.write(code+'\\n')\n",
    "  f.close()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0813c57-e965-4b2b-b644-28b1720aaffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#java_long数据集生成\n",
    "#对应“java_long”文件夹\n",
    "import pickle\n",
    "java_long_pkl0=open(r'.\\datasets\\java_long\\data0.pkl','rb')\n",
    "java_long_pkl1=open(r'.\\datasets\\java_long\\data1.pkl','rb')\n",
    "output_buggy='.\\java.buggy.long'\n",
    "output_fixxed='.\\java.fixed.long'\n",
    "pkl_content=pickle.load(java_long_pkl0)\n",
    "with open (output_buggy,'w',encoding='utf-8') as buggy ,open (output_fixxed,'w',encoding='utf-8') as fixed:\n",
    "  for example in pkl_content:\n",
    "    buggy_code=example['old']\n",
    "    fixed_code=example['new']\n",
    "    buggy_code=buggy_code.replace('\\n','')\n",
    "    buggy_code=buggy_code.replace('\\r','')\n",
    "    fixed_code=fixed_code.replace('\\n','')\n",
    "    fixed_code=fixed_code.replace('\\r','')\n",
    "    buggy.write(buggy_code+'\\n')\n",
    "    fixed.write(fixed_code+'\\n')\n",
    "\n",
    "pkl_content=pickle.load(java_long_pkl1)\n",
    "with open (output_buggy,'a',encoding='utf-8') as buggy ,open (output_fixxed,'a',encoding='utf-8') as fixed:\n",
    "  for example in pkl_content:\n",
    "    buggy_code=example['old']\n",
    "    fixed_code=example['new']\n",
    "    buggy_code=buggy_code.replace('\\n','')\n",
    "    buggy_code=buggy_code.replace('\\r','')\n",
    "    fixed_code=fixed_code.replace('\\n','')\n",
    "    fixed_code=fixed_code.replace('\\r','')\n",
    "    buggy.write(buggy_code+'\\n')\n",
    "    fixed.write(fixed_code+'\\n')\n",
    "pkl_content=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9aa171-751a-44c3-a6b9-aa7d3a382c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#java_mtufano数据集生成\n",
    "#对应“java_mtufano”文件夹\n",
    "import os\n",
    "import json\n",
    "#名如“000021ead7afe80b8eb1d34d61fbaf6d41f30555”等超多文件夹存放的目录\n",
    "code_datasets_path='.\\datasets\\java_mtufano\\bug-fixes-methods\\sciclone\\data10\\mtufano\\deepLearningMutants\\out\\changes\\code'\n",
    "#保存数据集的位置\n",
    "output_buggy='.\\java.buggy.mtufano'\n",
    "output_fixed='.\\java.fixed.mtufano'\n",
    "output_file_buggy=open(output_buggy,'w',encoding='utf-8')\n",
    "output_file_fixed=open(output_fixed,'w',encoding='utf-8')\n",
    "top_dirs_array=os.listdir(code_datasets_path)\n",
    "account_code=0\n",
    "for top_dir in top_dirs_array:\n",
    "  secondary_dirs_array=os.listdir(code_datasets_path+'\\\\'+top_dir)\n",
    "  for secondary_dir in secondary_dirs_array:\n",
    "    third_dirs_array=os.listdir(code_datasets_path+'\\\\'+top_dir+'\\\\'+secondary_dir)\n",
    "    for third_dir in third_dirs_array:\n",
    "      buggy_code_file_path=code_datasets_path+'\\\\'+top_dir+'\\\\'+secondary_dir+'\\\\'+third_dir+'\\\\before.java'\n",
    "      fixed_code_file_path=code_datasets_path+'\\\\'+top_dir+'\\\\'+secondary_dir+'\\\\'+third_dir+'\\\\after.java'\n",
    "      if not os.path.exists(buggy_code_file_path) or not os.path.exists(fixed_code_file_path):\n",
    "        continue\n",
    "      buggy_code_file=open(buggy_code_file_path,'r',encoding='utf-8')\n",
    "      fixed_code_file=open(fixed_code_file_path,'r',encoding='utf-8')\n",
    "      buggy_code=buggy_code_file.read()\n",
    "      fixed_code=fixed_code_file.read()\n",
    "      buggy_code=buggy_code.replace('\\n','')\n",
    "      buggy_code=buggy_code.replace('\\r','')\n",
    "      fixed_code=fixed_code.replace('\\n','')\n",
    "      fixed_code=fixed_code.replace('\\r','')\n",
    "      output_file_buggy.write(buggy_code+'\\n')\n",
    "      output_file_fixed.write(fixed_code+'\\n')\n",
    "      buggy_code_file.close()\n",
    "      fixed_code_file.close()\n",
    "      account_code=account_code+1\n",
    "      if account_code%10000==0:\n",
    "        print('Current progress:',account_code)\n",
    "\n",
    "print(account_code)\n",
    "output_file_buggy.close()\n",
    "output_file_fixed.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1899c82-f77c-4861-a66c-559cbe1b09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "对于上面得到的数据集，可自行挑选需要的部分组合，然后使用datasets_partitioning.ipynb拆分为训练集、验证集，用来预训练codebert和codegpt的交叉权重\n",
    "需要修改下方的参数\n",
    "#训练集文件位置\n",
    "train_buggy_file='train.buggy-fixed.buggy'\n",
    "train_fixed_file='train.buggy-fixed.fixed'\n",
    "#验证集文件位置\n",
    "valid_buggy_file='valid.buggy-fixed.buggy'\n",
    "valid_fixed_file='valid.buggy-fixed.fixed'\n",
    "\n",
    "训练完成后会在\n",
    "codebert_weight_path='.\\codebert'\n",
    "codegpt_weight_path='.\\codegpt'\n",
    "指定的文件夹内生成两个权重文件以及对应的配置文件，将这4个文件分别用来替换后续aecm模型的codebert和codegpt初始权重和配置文件\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221b2e2-fbf1-4a3b-9170-ba6b30835d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
